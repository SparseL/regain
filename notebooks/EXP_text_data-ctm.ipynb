{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models for Textual Data\n",
    "This shows how graphical models can be used to infer relationships between textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from regain.utils import flatten\n",
    "import numpy as np\n",
    "\n",
    "filename = \"../regain/data/text/webkb-train-stemmed.txt\"\n",
    "train = pd.read_csv(filename, header=None, sep='\\t', index_col=0).dropna()\n",
    "train.columns = ['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.unique(flatten([words.split(' ') for words in train.words.tolist() if isinstance(words, str)]))\n",
    "\n",
    "ld = [dict(zip(*np.unique(row.words.split(' '), return_counts=True))) for row in train.itertuples()\n",
    "      if isinstance(row.words, str)]\n",
    "\n",
    "X = pd.DataFrame(ld, index=[row.Index for row in train.itertuples()\n",
    "      if isinstance(row.words, str)]).fillna(0)\n",
    "\n",
    "y = X.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = train.words.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = pd.DataFrame(tf.todense(), index=train.index, columns=tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        topics.append(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print (documents[doc_index])\n",
    "    return topics\n",
    "# # NMF is able to use tf-idf\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# # Run NMF\n",
    "# nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "# nmf_W = nmf_model.transform(tfidf)\n",
    "# nmf_H = nmf_model.components_\n",
    "\n",
    "# print(\"NMF Topics\")\n",
    "# display_topics(nmf_H, nmf_W, tfidf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "n_top_words = 3\n",
    "n_top_documents = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run LDA\n",
    "# lda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online',\n",
    "#                                       learning_offset=50.,random_state=0).fit(tf)\n",
    "# lda_W = lda_model.transform(tf)\n",
    "# lda_H = lda_model.components_\n",
    "\n",
    "# print(\"LDA Topics\")\n",
    "# topics = display_topics(lda_H, lda_W, tf_vectorizer.get_feature_names(), documents, n_top_words, n_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(lda_W, index=train.index, columns=topics)\n",
    "# X = lda_W\n",
    "# y = documents.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.DataFrame([], columns=words)\n",
    "# y = []\n",
    "# for row in train.itertuples():\n",
    "#     if isinstance(row.words, str):\n",
    "#         series = pd.Series(dict(zip(*np.unique(row.words.split(' '), return_counts=True))), name=row.Index)\n",
    "#         X = X.append(series)\n",
    "#         y.append(row.Index)\n",
    "\n",
    "# X = X.fillna(0)\n",
    "# X.index = y\n",
    "\n",
    "# y = np.asarray(y)\n",
    "\n",
    "words_to_discard = []\n",
    "for yy in np.unique(y):\n",
    "    words_to_discard += list(X[words[X[y==yy].sum(axis=0) == 0]].columns)\n",
    "\n",
    "words_to_keep = list(set(X.columns) - set(words_to_discard))\n",
    "\n",
    "def logentropy_normalize(X):\n",
    "    P = X / X.values.sum(axis=0, keepdims=True)\n",
    "    E = 1 + (P * np.log(P)).fillna(0).values.sum(axis=0, keepdims=True) / np.log(X.shape[0] + 1)\n",
    "    return E * np.log(1 + X)\n",
    "\n",
    "X_new = logentropy_normalize(X)[words_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LogEntropyModel\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "corp = [w.split(' ') for w in train.words.tolist() if isinstance(w, str)]\n",
    "text = corp #common_texts\n",
    "dct = Dictionary(text)  # fit dictionary\n",
    "\n",
    "num_terms = 50 # or words.size\n",
    "dct.filter_extremes(keep_n=num_terms)\n",
    "\n",
    "corpus = [dct.doc2bow(row) for row in text][:10]  #convert to BoW format\n",
    "model = LogEntropyModel(corpus, normalize=True)  # fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ctmmodel; reload(ctmmodel)\n",
    "ctm_model = ctmmodel.CtmModel(corpus, id2word=dct, num_topics=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for c in corpus:\n",
    "    doc_words = []\n",
    "    for cc in c:\n",
    "        doc_words.extend([dct[cc[0]]] * cc[1])\n",
    "    all_words.append(' '.join(doc_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyctm; reload(pyctm)\n",
    "from pyctm import variational_bayes, inferencer\n",
    "reload(variational_bayes); reload(inferencer)\n",
    "\n",
    "# parameter set 3\n",
    "alpha_mu=0.\n",
    "alpha_sigma=1\n",
    "alpha_beta=0\n",
    "\n",
    "ctm_inferencer = variational_bayes.VariationalBayes();\n",
    "ctm_inferencer._initialize(all_words, list(dct.values()), number_of_topics=15,\n",
    "                           alpha_mu=alpha_mu, alpha_sigma=alpha_sigma, alpha_beta=alpha_beta);\n",
    "\n",
    "for iteration in range(50):\n",
    "    ctm_inferencer.learning(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logl, lamda, nu = ctm_inferencer.inference(all_words)\n",
    "\n",
    "ll = utils.topic_beta(ctm_inferencer)\n",
    "\n",
    "topic_words = pd.DataFrame(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_str_repr = []\n",
    "for row in topic_words.iterrows():\n",
    "    print(pd.DataFrame(row[1].sort_values(ascending=False)[:3]).T)\n",
    "    topic_str_repr.append(' '.join(row[1].sort_values(ascending=False)[:3].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_dct_values = list(dct.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.DataFrame(ctm_model.beta, columns=word_dct_values)\n",
    "dff = dff[sorted(dff.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim import models\n",
    "# model = models.LdaModel(corpus, id2word=dct, num_topics=num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "XX = gensim.matutils.corpus2dense(model[corpus], num_terms=num_terms).T\n",
    "cols = list(dct.values())\n",
    "df = pd.DataFrame(XX, columns=cols, index=y)\n",
    "# df[words_to_keep].T.sort_index().T\n",
    "\n",
    "# X = df[words_to_keep].values\n",
    "X = df.values\n",
    "y = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from regain.covariance import kernel_time_graphical_lasso_\n",
    "from regain.model_selection import stability_optimization\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "mdl = kernel_time_graphical_lasso_.KernelTimeGraphicalLasso(\n",
    "    verbose=0, kernel=np.ones((np.unique(y).size, np.unique(y).size)), psi='l1',\n",
    "    alpha=0.45, max_iter=1000).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socv = stability_optimization.GraphicalModelStabilitySelection(\n",
    "    mdl, param_grid=dict(alpha=np.logspace(2, -2)), cv=StratifiedShuffleSplit(100)\n",
    ").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_times = np.unique(y).size\n",
    "n_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.triu_indices(n_dim, 1)\n",
    "dof = idx[0].size * n_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain import utils\n",
    "# utils.save_pickle(socv, \"socv\")\n",
    "\n",
    "socv = utils.load_pickle(\"socv.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = socv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nonzero percentage: %.4f\" % (np.sum([np.count_nonzero(P[idx]) for P in mdl.precision_]) / dof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from kdge import plot_plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "py.init_notebook_mode()\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mdl.precision_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.utils import retain_top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = -1\n",
    "trace = []\n",
    "graphs = []\n",
    "# for i, p in enumerate(ltgl.precision_ - ltgl.latent_):\n",
    "for i, p in enumerate(mdl.precision_):\n",
    "\n",
    "    A = np.abs(p - np.diag(np.diag(p)))\n",
    "    A = retain_top_n(A, top_n)\n",
    "    G = nx.from_numpy_matrix(A * 0.00001)\n",
    "    graphs.append(G)\n",
    "    trace.append(pl.plot_circular(G, df.columns, 1.4, cmap='Blues',\n",
    "                                  #color_nodes=plt.rcParams['axes.prop_cycle'].by_key()['color'][:5]\n",
    "                                 ))\n",
    "\n",
    "fig = tools.make_subplots(\n",
    "    rows=1, cols=len(mdl.precision_), horizontal_spacing=.1, print_grid=False)\n",
    "\n",
    "for j, tr in enumerate(trace):\n",
    "    tr['data'][k]['xaxis'] = 'x' + str(j+1)\n",
    "    tr['data'][k]['yaxis'] = 'y' + str(j+1)\n",
    "\n",
    "for j, tr in enumerate(trace):\n",
    "    for i, x in enumerate(tr['data']):\n",
    "        col = j + 1\n",
    "        x['legendgroup'] = 'group'+ str(j+1)\n",
    "        x['showlegend'] = False\n",
    "        fig.append_trace(x,1,col)\n",
    "        \n",
    "\n",
    "\n",
    "for j, tr in enumerate(trace):\n",
    "    fig.layout.annotations += tuple([pl._set_ref(\n",
    "        x, 'x'+ str(j+1), 'y'+ str(j+1)) for x in tr['layout']['annotations']])\n",
    "\n",
    "    fig['layout']['xaxis'+str(j+1)].update(showgrid=False, zeroline=False, showticklabels=False)\n",
    "    fig['layout']['yaxis'+str(j+1)].update(showgrid=False, zeroline=False, showticklabels=False)\n",
    "\n",
    "py.init_notebook_mode()\n",
    "\n",
    "fig['layout'].update(height=900, width=4000,hovermode='closest',\n",
    "                     paper_bgcolor='rgba(0,0,0,0)',\n",
    "                        plot_bgcolor='rgba(0,0,0,0)')\n",
    "# fig.layout.annotations += tuple([dict(\n",
    "#     text=\"Python code: <a href='https://plot.ly/ipython-notebooks/network-graphs/'> https://plot.ly/ipython-notebooks/network-graphs/</a>\",\n",
    "#     showarrow=False, xref=\"paper\", yref=\"paper\", x=0.005, y=-0.2)])\n",
    "# fig['layout'].update(scene=dict(aspectmode=\"data\"))\n",
    "py.iplot(fig)\n",
    "# py.offline.iplot(fig, filename='figure_factory_subplot', image='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_image(fig, \"graphs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyctm\n",
    "\n",
    "from pyctm import variational_bayes, inferencer\n",
    "\n",
    "# parameter set 3\n",
    "alpha_mu=0.\n",
    "alpha_sigma=.1\n",
    "alpha_beta=-.1\n",
    "\n",
    "ctm_inferencer = variational_bayes.VariationalBayes();\n",
    "ctm_inferencer._initialize(train.words.tolist(), words, 20, alpha_mu, alpha_sigma, alpha_beta);\n",
    "\n",
    "for iteration in range(50):\n",
    "    ctm_inferencer.learning(-1)\n",
    "\n",
    "logl, lamda, nu = ctm_inferencer.inference(train.words.tolist())\n",
    "\n",
    "ll = utils.topic_beta(ctm_inferencer)\n",
    "\n",
    "topic_words = pd.DataFrame(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(lamda).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctm import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "\n",
    "from regain import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_pickle(lamda, \"lambda_ctm.pkl\")\n",
    "lamda = utils.load_pickle('lambda_ctm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl = GraphicalLassoCV().fit(lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gl.precision_\n",
    "\n",
    "A = np.abs(p - np.diag(np.diag(p)))\n",
    "A = retain_top_n(A, 20)\n",
    "G = nx.from_numpy_matrix(A * 3)\n",
    "fig = pl.plot_circular(G, topic_str_repr, 2, cmap='Blues')\n",
    "\n",
    "fig['layout'].update(height=800, width=800,hovermode='closest',\n",
    "                     paper_bgcolor='rgba(0,0,0,0)',\n",
    "                        plot_bgcolor='rgba(0,0,0,0)')\n",
    "# fig.layout.annotations += tuple([dict(\n",
    "#     text=\"Python code: <a href='https://plot.ly/ipython-notebooks/network-graphs/'> https://plot.ly/ipython-notebooks/network-graphs/</a>\",\n",
    "#     showarrow=False, xref=\"paper\", yref=\"paper\", x=0.005, y=-0.2)])\n",
    "# fig['layout'].update(scene=dict(aspectmode=\"data\"))\n",
    "py.iplot(fig)\n",
    "# py.offline.iplot(fig, filename='figure_factory_subplot', image='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
