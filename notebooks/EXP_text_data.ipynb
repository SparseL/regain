{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models for Textual Data\n",
    "This shows how graphical models can be used to infer relationships between textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from regain.utils import flatten\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "from regain.data import base; import imp; imp.reload(base)\n",
    "train, test = base.load_webkb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - create data manually (no filtering on terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dictionary of the words (only strings)\n",
    "words = np.unique(flatten([words.split(' ') for words in train.words.tolist() if isinstance(words, str)]))\n",
    "\n",
    "# for each document, create the TermFrequency\n",
    "tf_docs = [dict(Counter(v.split(' '))) for k, v in train.itertuples() if isinstance(v, str)]\n",
    "assert len(tf_docs) == len(train)\n",
    "\n",
    "# build data\n",
    "X = pd.DataFrame(tf_docs, index=[row.Index for row in train.itertuples()\n",
    "      if isinstance(row.words, str)]).fillna(0)\n",
    "y = X.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - create data with some filters based on sklearn-`CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=1., min_df=0.) #, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(train.values.flatten())\n",
    "\n",
    "df_tf = pd.DataFrame(tf.todense(), index=train.index, columns=tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X, df_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NMF is able to use tf-idf\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# # Run NMF\n",
    "# nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "# nmf_W = nmf_model.transform(tfidf)\n",
    "# nmf_H = nmf_model.components_\n",
    "\n",
    "# print(\"NMF Topics\")\n",
    "# display_topics(nmf_H, nmf_W, tfidf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online',\n",
    "                                      learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics\n",
      "Topic 0: network (0.123) comput (0.104) research (0.098) paper (0.086)\n",
      "Topic 1: austin (342.521) texa (185.231) utexa (150.192) alberta (132.321)\n",
      "Topic 2: comput (381.630) scienc (352.159) robot (336.811) depart (216.900)\n",
      "Topic 3: assign (0.098) mark (0.076) email (0.069) jonathan (0.063)\n",
      "Topic 4: robbin (8.227) suni (5.420) flame (3.735) zhang (2.413)\n",
      "Topic 5: pradesh (3.576) andhra (3.092) pankaj (2.803) hail (2.232)\n",
      "Topic 6: homepag (45.555) game (15.986) iowa (8.438) othello (7.967)\n",
      "Topic 7: linear (55.510) nonlinear (35.098) optim (29.788) mangasarian (27.749)\n",
      "Topic 8: logic (145.654) china (35.050) wesleyan (29.713) file (29.278)\n",
      "Topic 9: alistair (16.705) markov (15.683) clair (7.828) randal (5.716)\n",
      "Topic 10: frame (113.834) browser (33.161) nmsu (33.044) mexico (28.437)\n",
      "Topic 11: washington (296.118) seattl (107.628) bershad (61.874) sieg (48.006)\n",
      "Topic 12: dsp (32.210) xxx (15.071) clayton (3.662) stoll (2.317)\n",
      "Topic 13: swanson (0.738) lab (0.117) homework (0.109) iram (0.092)\n",
      "Topic 14: univers (0.083) comput (0.083) scienc (0.073) histori (0.069)\n",
      "Topic 15: co (29.423) main (10.425) orono (3.905) jonathan (3.363)\n",
      "Topic 16: julia (0.721) continu (0.057) cornel (0.050) notwithstand (0.035)\n",
      "Topic 17: cheap (1.866) gideon (1.615) surgeri (1.135) afford (0.709)\n",
      "Topic 18: section (0.121) time (0.108) system (0.101) discret (0.077)\n",
      "Topic 19: document (95.423) move (32.876) perman (13.469) page (0.084)\n",
      "Topic 20: proceed (0.072) comput (0.058) bound (0.054) symposium (0.052)\n",
      "Topic 21: page (2082.828) program (1638.825) assign (1436.122) comput (1380.512)\n",
      "Topic 22: graphic (0.057) activ (0.052) comput (0.052) system (0.052)\n",
      "Topic 23: cse (143.037) ci (38.734) upenn (21.857) chien (16.184)\n",
      "Topic 24: mitchel (32.020) imielinski (21.274) berman (15.914) badrinath (13.978)\n",
      "Topic 25: plan (35.729) train (33.203) episod (20.521) diwan (17.536)\n",
      "Topic 26: comput (0.084) assign (0.060) system (0.059) program (0.053)\n",
      "Topic 27: system (0.051) real (0.049) time (0.044) mok (0.038)\n",
      "Topic 28: kfouri (13.099) assaf (9.520) coordin (8.133) lab (0.215)\n",
      "Topic 29: music (54.914) sound (29.686) atmospher (14.669) spatial (14.325)\n",
      "Topic 30: project (0.055) system (0.052) java (0.051) section (0.045)\n",
      "Topic 31: elli (8.949) carla (5.679) mobil (3.886) surendar (2.705)\n",
      "Topic 32: gun (351.456) state (239.847) peopl (233.535) law (225.918)\n",
      "Topic 33: comput (0.073) scienc (0.071) bound (0.063) proceed (0.062)\n",
      "Topic 34: cornel (154.460) greg (19.432) miller (15.680) ru (14.153)\n",
      "Topic 35: homework (0.064) comput (0.062) vol (0.059) lab (0.057)\n",
      "Topic 36: databas (401.363) queri (234.941) data (133.661) sequenc (98.805)\n",
      "Topic 37: madison (191.023) wisconsin (152.197) wisc (101.812) dir (61.005)\n",
      "Topic 38: music (0.125) comput (0.101) system (0.085) softwar (0.084)\n",
      "Topic 39: home (0.069) page (0.061) depart (0.047) teach (0.047)\n",
      "Topic 40: method (53.357) equat (44.375) numer (40.341) spars (32.743)\n",
      "Topic 41: comput (6368.884) system (3885.242) scienc (3331.405) univers (3248.448)\n",
      "Topic 42: spencer (2.478) alon (2.451) leisur (1.667) ohio (0.963)\n",
      "Topic 43: shapiro (33.807) acap (23.795) mcgill (23.072) marc (7.730)\n",
      "Topic 44: superscalar (12.170) tullsen (6.522) egger (2.304) pipelin (1.611)\n",
      "Topic 45: perform (0.045) comput (0.044) share (0.043) foster (0.041)\n",
      "Topic 46: laru (0.038) program (0.035) jame (0.035) reseach (0.034)\n",
      "Topic 47: bound (25.310) beam (17.567) impagliazzo (14.698) lower (14.474)\n",
      "Topic 48: harvard (121.525) stuart (29.435) kung (28.205) da (26.885)\n",
      "Topic 49: unc (24.880) loui (17.434) ronald (10.073) eiffel (8.024)\n"
     ]
    }
   ],
   "source": [
    "from regain import utils_text; imp.reload(utils_text)\n",
    "\n",
    "print(\"LDA Topics\")\n",
    "topics = utils_text.display_topics(lda_H, lda_W, tf_vectorizer.get_feature_names(),\n",
    "                                   train.values.flatten(), n_top_words=4, n_top_documents=1, print_docs=False)\n",
    "\n",
    "df = pd.DataFrame(lda_W, index=train.index, columns=topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogEntropyModel (`regain`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_discard = []\n",
    "for yy in np.unique(y):\n",
    "    words_to_discard += list(X[words[X[y==yy].sum(axis=0) == 0]].columns)\n",
    "\n",
    "words_to_keep = list(set(X.columns) - set(words_to_discard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/federicot/src/fdtomasi/regain/regain/utils_text.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "  E = 1 + (P * np.log(P)).fillna(0).values.sum(\n"
     ]
    }
   ],
   "source": [
    "X_new = utils_text.logentropy_normalize(X)[words_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogEntropyModel (`gensim`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LogEntropyModel, LdaModel\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "corp = [w.split(' ') for w in train.words if isinstance(w, str)]\n",
    "text = corp #common_texts\n",
    "dct = Dictionary(text)  # fit dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_terms = 50 # or words.size\n",
    "dct.filter_extremes(keep_n=num_terms)\n",
    "\n",
    "corpus = [dct.doc2bow(row) for row in text]  #convert to BoW format\n",
    "model = LogEntropyModel(corpus, normalize=True)  # fit model\n",
    "\n",
    "# model = models.LdaModel(corpus, id2word=dct, num_topics=num_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "X_new = gensim.matutils.corpus2dense(model[corpus], num_terms=num_terms).T\n",
    "df_new = pd.DataFrame(X_new, columns=list(dct.values()), index=y)\n",
    "\n",
    "# df[words_to_keep].T.sort_index().T\n",
    "# X = df[words_to_keep].values\n",
    "\n",
    "# X = df.values\n",
    "# y = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from regain.covariance import kernel_time_graphical_lasso_\n",
    "from regain.model_selection import stability_optimization\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "mdl = kernel_time_graphical_lasso_.KernelTimeGraphicalLasso(\n",
    "    verbose=0, kernel=np.ones((np.unique(y).size, np.unique(y).size)), psi='l1',\n",
    "    alpha=0.45, max_iter=1000).fit(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(stability_optimization);\n",
    "socv = stability_optimization.GraphicalModelStabilitySelection(\n",
    "    mdl, param_grid=dict(alpha=np.logspace(2, -2)),\n",
    "    sampling_size=200,\n",
    ").fit(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_times = np.unique(y).size\n",
    "n_dim = X_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.triu_indices(n_dim, 1)\n",
    "dof = idx[0].size * n_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = socv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero percentage: 0.7133\n"
     ]
    }
   ],
   "source": [
    "print(\"Nonzero percentage: %.4f\" % (np.sum([np.count_nonzero(P[idx]) for P in mdl.precision_]) / dof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from kdge import plot_plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "py.init_notebook_mode()\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mdl.precision_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.utils import retain_top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = -1\n",
    "trace = []\n",
    "graphs = []\n",
    "# for i, p in enumerate(ltgl.precision_ - ltgl.latent_):\n",
    "for i, p in enumerate(mdl.precision_):\n",
    "\n",
    "    A = np.abs(p - np.diag(np.diag(p)))\n",
    "    A = retain_top_n(A, top_n)\n",
    "    G = nx.from_numpy_matrix(A * 0.00001)\n",
    "    graphs.append(G)\n",
    "    trace.append(pl.plot_circular(G, df.columns, 1.4, cmap='Blues',\n",
    "                                  #color_nodes=plt.rcParams['axes.prop_cycle'].by_key()['color'][:5]\n",
    "                                 ))\n",
    "\n",
    "fig = tools.make_subplots(\n",
    "    rows=1, cols=len(mdl.precision_), horizontal_spacing=.1, print_grid=False)\n",
    "\n",
    "for j, tr in enumerate(trace):\n",
    "    tr['data'][k]['xaxis'] = 'x' + str(j+1)\n",
    "    tr['data'][k]['yaxis'] = 'y' + str(j+1)\n",
    "\n",
    "for j, tr in enumerate(trace):\n",
    "    for i, x in enumerate(tr['data']):\n",
    "        col = j + 1\n",
    "        x['legendgroup'] = 'group'+ str(j+1)\n",
    "        x['showlegend'] = False\n",
    "        fig.append_trace(x,1,col)\n",
    "        \n",
    "\n",
    "\n",
    "for j, tr in enumerate(trace):\n",
    "    fig.layout.annotations += tuple([pl._set_ref(\n",
    "        x, 'x'+ str(j+1), 'y'+ str(j+1)) for x in tr['layout']['annotations']])\n",
    "\n",
    "    fig['layout']['xaxis'+str(j+1)].update(showgrid=False, zeroline=False, showticklabels=False)\n",
    "    fig['layout']['yaxis'+str(j+1)].update(showgrid=False, zeroline=False, showticklabels=False)\n",
    "\n",
    "py.init_notebook_mode()\n",
    "\n",
    "fig['layout'].update(height=900, width=4000,hovermode='closest',\n",
    "                     paper_bgcolor='rgba(0,0,0,0)',\n",
    "                        plot_bgcolor='rgba(0,0,0,0)')\n",
    "# fig.layout.annotations += tuple([dict(\n",
    "#     text=\"Python code: <a href='https://plot.ly/ipython-notebooks/network-graphs/'> https://plot.ly/ipython-notebooks/network-graphs/</a>\",\n",
    "#     showarrow=False, xref=\"paper\", yref=\"paper\", x=0.005, y=-0.2)])\n",
    "# fig['layout'].update(scene=dict(aspectmode=\"data\"))\n",
    "py.iplot(fig)\n",
    "# py.offline.iplot(fig, filename='figure_factory_subplot', image='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_image(fig, \"graphs.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctmmodel\n",
    "ctm_model = ctmmodel.CtmModel(corpus, id2word=dct, num_topics=15)\n",
    "\n",
    "all_words = []\n",
    "for c in corpus:\n",
    "    doc_words = []\n",
    "    for cc in c:\n",
    "        doc_words.extend([dct[cc[0]]] * cc[1])\n",
    "    all_words.append(' '.join(doc_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyctm\n",
    "from pyctm import variational_bayes, inferencer, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter set 3\n",
    "alpha_mu=0.\n",
    "alpha_sigma=1\n",
    "alpha_beta=0\n",
    "\n",
    "ctm_inferencer = variational_bayes.VariationalBayes();\n",
    "ctm_inferencer._initialize(\n",
    "    all_words, list(dct.values()),\n",
    "    number_of_topics=15,\n",
    "    alpha_mu=alpha_mu, alpha_sigma=alpha_sigma,\n",
    "    alpha_beta=alpha_beta);\n",
    "# ctm_inferencer._initialize(train.words.tolist(), words, 20,\n",
    "#                            alpha_mu, alpha_sigma, alpha_beta);\n",
    "\n",
    "for iteration in range(50):\n",
    "    ctm_inferencer.learning(-1)\n",
    "\n",
    "logl, lamda, nu = ctm_inferencer.inference(all_words)\n",
    "# logl, lamda, nu = ctm_inferencer.inference(train.words.tolist())\n",
    "\n",
    "ll = utils.topic_beta(ctm_inferencer)\n",
    "\n",
    "topic_words = pd.DataFrame(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_str_repr = []\n",
    "for row in topic_words.iterrows():\n",
    "    print(pd.DataFrame(row[1].sort_values(ascending=False)[:3]).T)\n",
    "    topic_str_repr.append(' '.join(row[1].sort_values(ascending=False)[:3].index))\n",
    "\n",
    "word_dct_values = list(dct.values())\n",
    "\n",
    "dff = pd.DataFrame(ctm_model.beta, columns=word_dct_values)\n",
    "dff = dff[sorted(dff.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "\n",
    "from regain import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl = GraphicalLassoCV().fit(lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gl.precision_\n",
    "\n",
    "A = np.abs(p - np.diag(np.diag(p)))\n",
    "A = retain_top_n(A, 20)\n",
    "G = nx.from_numpy_matrix(A * 3)\n",
    "fig = pl.plot_circular(G, topic_str_repr, 2, cmap='Blues')\n",
    "\n",
    "fig['layout'].update(height=800, width=800,hovermode='closest',\n",
    "                     paper_bgcolor='rgba(0,0,0,0)',\n",
    "                        plot_bgcolor='rgba(0,0,0,0)')\n",
    "# fig.layout.annotations += tuple([dict(\n",
    "#     text=\"Python code: <a href='https://plot.ly/ipython-notebooks/network-graphs/'> https://plot.ly/ipython-notebooks/network-graphs/</a>\",\n",
    "#     showarrow=False, xref=\"paper\", yref=\"paper\", x=0.005, y=-0.2)])\n",
    "# fig['layout'].update(scene=dict(aspectmode=\"data\"))\n",
    "py.iplot(fig)\n",
    "# py.offline.iplot(fig, filename='figure_factory_subplot', image='svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
